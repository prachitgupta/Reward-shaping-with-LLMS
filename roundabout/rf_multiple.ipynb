{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prachit/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data merged and sorted successfully. Saved to datasets_try/collision_free.csv\n"
     ]
    }
   ],
   "source": [
    "def merge_and_sort_csv(file1, file2, output_file):\n",
    "    # Load the CSV files into pandas DataFrames\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    #df3 = pd.read_csv(file3)\n",
    "\n",
    "    # Combine the data from both DataFrames\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # Sort the combined data by the 'action' column (ascending order)\n",
    "    sorted_df = combined_df.sort_values(by='action', ascending=True)\n",
    "\n",
    "    # Save the sorted data to a new CSV file\n",
    "    sorted_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Data merged and sorted successfully. Saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "file1 = 'datasets_try/processed_features_ml_cl.csv'  # Path to your first CSV file\n",
    "file2 = 'datasets_try/processed_features_all_cl.csv'  # Path to your second CSV file\n",
    "#file3 = 'datasets_try/processed_features_5k.csv'\n",
    "# file4 = 'datasets_try/processed_features_el.csv' \n",
    "output_file = 'datasets_try/collision_free.csv'  # Output CSV file\n",
    "\n",
    "merge_and_sort_csv(file1, file2, output_file)\n",
    "\n",
    "##add more datapoints \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0    2761\n",
      "4.0    2107\n",
      "0.0    1605\n",
      "1.0    1210\n",
      "2.0      17\n",
      "Name: action, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "collision_free_data = pd.read_csv('datasets_try/processed_features_collision_free.csv')\n",
    "\n",
    "# Check the distribution of the 'action' column\n",
    "action_distribution = collision_free_data['action'].value_counts()\n",
    "print(action_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPSAMPLE MINORITY using dataset fromrandomly generated cenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved the data. Merged dataset saved at datasets_try/collision_free_upsampled.csv\n",
      "3.0    2761\n",
      "4.0    2107\n",
      "0.0    1605\n",
      "1.0    1210\n",
      "2.0      76\n",
      "Name: action, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Load the dataset from the file\n",
    "file_path =  'datasets_try/processed_features_all.csv'\n",
    "#file_path2 = 'datasets_try/processed_features_el_cl.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 2: Sort the dataset by the action label (last column)\n",
    "# Assuming the action label is the last column\n",
    "df_sorted = df.sort_values(by='action', ascending=True)\n",
    "\n",
    "# Step 3: Extract data points with action label 0 and 2\n",
    "df_label_0 = df_sorted[df_sorted[df.columns[-1]] == 0]\n",
    "df_label_2 = df_sorted[df_sorted[df.columns[-1]] == 2]\n",
    "\n",
    "# Step 4: Save the filtered data to separate CSV files\n",
    "file_label_0 = 'datasets_try/processed_label_0_data.csv'\n",
    "file_label_2 = 'datasets_try/processed_label_2_data.csv'\n",
    "\n",
    "df_label_0.to_csv(file_label_0, index=False)\n",
    "df_label_2.to_csv(file_label_2, index=False)\n",
    "\n",
    "# Step 5: Extract 200 data points from each new CSV and merge them into another CSV\n",
    "# Load the previously saved files\n",
    "df_label_0_loaded = pd.read_csv(file_label_0)\n",
    "df_label_2_loaded = pd.read_csv(file_label_2)\n",
    "\n",
    "# Randomly select 200 data points from each\n",
    "df_label_0_sampled = df_label_0_loaded.sample(n=0, random_state=42)\n",
    "df_label_2_sampled = df_label_2_loaded.sample(n=59, random_state=42)\n",
    "\n",
    "# Step 6: Merge the two sampled dataframes\n",
    "merged_df = pd.concat([df_label_0_sampled, df_label_2_sampled])\n",
    "\n",
    "# Step 7: Load the existing merged dataset if it exists or create a new one\n",
    "input_file_path =  'datasets_try/processed_features_collision_free.csv'\n",
    "\n",
    "try:\n",
    "    collision_free_data = pd.read_csv(input_file_path)\n",
    "    # Append the new merged data to the existing one\n",
    "    collision_free_data_upsampled = pd.concat([collision_free_data, merged_df])\n",
    "except FileNotFoundError:\n",
    "    # If the file does not exist, create a new one\n",
    "    collision_free_data = merged_df\n",
    "\n",
    "# Step 8: Save the merged data back into the CSV\n",
    "collision_free_data_upsampled.to_csv('datasets_try/processed_collision_free_upsampled.csv', index=False)\n",
    "\n",
    "print(f\"Successfully processed and saved the data. Merged dataset saved at datasets_try/collision_free_upsampled.csv\") \n",
    "\n",
    "action_distribution = collision_free_data_upsampled['action'].value_counts()\n",
    "print(action_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Classifier Accuracy on Test Data: 99.40%\n",
      "Precision on Test Data: 99.65%\n",
      "Recall on Test Data: 99.74%\n",
      "F1-Score on Test Data: 99.70%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.56      0.59         9\n",
      "           1       1.00      1.00      1.00      1155\n",
      "\n",
      "    accuracy                           0.99      1164\n",
      "   macro avg       0.81      0.78      0.79      1164\n",
      "weighted avg       0.99      0.99      0.99      1164\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   5    4]\n",
      " [   3 1152]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models_try/binary_rf_model_collision_free_upsampled.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a binary target variable\n",
    "collision_free_data_upsampled['binary_target'] = collision_free_data_upsampled['action'].apply(lambda x: 1 if x in [1,3,0, 4] else 0)\n",
    "\n",
    "# Split the data into features and binary target\n",
    "X_binary = collision_free_data_upsampled.drop(columns=['action', 'binary_target'])\n",
    "y_binary = collision_free_data_upsampled['binary_target']\n",
    "\n",
    "# Split the dataset into train, test, and eval sets\n",
    "X_train_binary, X_temp_binary, y_train_binary, y_temp_binary = train_test_split(X_binary, y_binary, test_size=0.3, random_state=42)\n",
    "X_test_binary, X_eval_binary, y_test_binary, y_eval_binary = train_test_split(X_temp_binary, y_temp_binary, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train the binary classifier\n",
    "binary_rf_model = RandomForestClassifier(random_state=42)\n",
    "binary_rf_model.fit(X_train_binary, y_train_binary)\n",
    "\n",
    "# Evaluate the binary classifier\n",
    "y_pred_binary_test = binary_rf_model.predict(X_test_binary)\n",
    "accuracy_binary_test = accuracy_score(y_test_binary, y_pred_binary_test)\n",
    "print(f\"Binary Classifier Accuracy on Test Data: {accuracy_binary_test * 100:.2f}%\")\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision_test = precision_score(y_test_binary, y_pred_binary_test)\n",
    "recall_test = recall_score(y_test_binary, y_pred_binary_test)\n",
    "f1_test = f1_score(y_test_binary, y_pred_binary_test)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Precision on Test Data: {precision_test * 100:.2f}%\")\n",
    "print(f\"Recall on Test Data: {recall_test * 100:.2f}%\")\n",
    "print(f\"F1-Score on Test Data: {f1_test * 100:.2f}%\")\n",
    "\n",
    "# Generate a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_binary, y_pred_binary_test))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_binary, y_pred_binary_test))\n",
    "\n",
    "# Save the binary classifier\n",
    "joblib.dump(binary_rf_model, 'models_try/binary_rf_model_collision_free_upsampled.pkl')\n",
    "#print(data[5:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0    1567\n",
      "1.0    1500\n",
      "3.0    1424\n",
      "2.0     611\n",
      "0.0     506\n",
      "Name: action, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "collision_free_data_upsampled['binary_target'] = collision_free_data_upsampled['action'].apply(lambda x: 1 if x in [1, 3,4] else 0)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = collision_free_data_upsampled[collision_free_data_upsampled['binary_target'] == 1]\n",
    "minority_class = collision_free_data_upsampled[collision_free_data_upsampled['binary_target'] == 0]\n",
    "idle_class = collision_free_data_upsampled[collision_free_data_upsampled['action'] == 1]\n",
    "non_ideal_class = collision_free_data_upsampled[collision_free_data_upsampled['action'] != 1]\n",
    "\n",
    "# Downsample majority class\n",
    "idle_downsampled = resample(idle_class, \n",
    "                                replace=True,    # Sample without replacement\n",
    "                                n_samples=1500,  # Match minority class size\n",
    "                                random_state=42)\n",
    "collision_free_data_up_downsampled = pd.concat([idle_downsampled, non_ideal_class])\n",
    "\n",
    "print(collision_free_data_up_downsampled['action'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary target variable\n",
    "collision_free_data_up_downsampled['binary_target'] = collision_free_data_up_downsampled['action'].apply(lambda x: 1 if x in [1,3, 4] else 0)\n",
    "\n",
    "# Split the data into features and binary target\n",
    "X_binary = collision_free_data_up_downsampled.drop(columns=['action', 'binary_target'])\n",
    "y_binary = collision_free_data_up_downsampled['binary_target']\n",
    "\n",
    "# Split the dataset into train, test, and eval sets\n",
    "X_train_binary, X_temp_binary, y_train_binary, y_temp_binary = train_test_split(X_binary, y_binary, test_size=0.2, random_state=42)\n",
    "X_test_binary, X_eval_binary, y_test_binary, y_eval_binary = train_test_split(X_temp_binary, y_temp_binary, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train the binary classifier\n",
    "binary_rf_model = RandomForestClassifier(random_state=42)\n",
    "binary_rf_model.fit(X_train_binary, y_train_binary)\n",
    "\n",
    "# Evaluate the binary classifier\n",
    "y_pred_binary_test = binary_rf_model.predict(X_test_binary)\n",
    "accuracy_binary_test = accuracy_score(y_test_binary, y_pred_binary_test)\n",
    "print(f\"Binary Classifier Accuracy on Test Data: {accuracy_binary_test * 100:.2f}%\")\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision_test = precision_score(y_test_binary, y_pred_binary_test)\n",
    "recall_test = recall_score(y_test_binary, y_pred_binary_test)\n",
    "f1_test = f1_score(y_test_binary, y_pred_binary_test)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Precision on Test Data: {precision_test * 100:.2f}%\")\n",
    "print(f\"Recall on Test Data: {recall_test * 100:.2f}%\")\n",
    "print(f\"F1-Score on Test Data: {f1_test * 100:.2f}%\")\n",
    "\n",
    "# Generate a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_binary, y_pred_binary_test))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_binary, y_pred_binary_test))\n",
    "\n",
    "# Save the binary classifier\n",
    "joblib.dump(binary_rf_model, 'models_try/binary_rf_model_collision_free_up_down.pkl')\n",
    "#print(data[5:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train_binary, y_train_binary)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test = xgb_model.predict(X_test_binary)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_test = accuracy_score(y_test_binary, y_pred_test)\n",
    "precision_test = precision_score(y_test_binary, y_pred_test)\n",
    "recall_test = recall_score(y_test_binary, y_pred_test)\n",
    "f1_test = f1_score(y_test_binary, y_pred_test)\n",
    "\n",
    "print(f\"XGBoost Accuracy on Test Data: {accuracy_test * 100:.2f}%\")\n",
    "print(f\"Precision on Test Data: {precision_test * 100:.2f}%\")\n",
    "print(f\"Recall on Test Data: {recall_test * 100:.2f}%\")\n",
    "print(f\"F1-Score on Test Data: {f1_test * 100:.2f}%\")\n",
    "\n",
    "# Generate a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_binary, y_pred_test))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_binary, y_pred_test))\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "joblib.dump(xgb_model, 'models_try/xgb_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SVM classifier\n",
    "svm_model = SVC(kernel='rbf', random_state=42)  # Using radial basis function kernel\n",
    "\n",
    "# Train the model\n",
    "svm_model.fit(X_train_binary, y_train_binary)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test = svm_model.predict(X_test_binary)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_test = accuracy_score(y_test_binary, y_pred_test)\n",
    "precision_test = precision_score(y_test_binary, y_pred_test)\n",
    "recall_test = recall_score(y_test_binary, y_pred_test)\n",
    "f1_test = f1_score(y_test_binary, y_pred_test)\n",
    "\n",
    "print(f\"svm Accuracy on Test Data: {accuracy_test * 100:.2f}%\")\n",
    "print(f\"Precision on Test Data: {precision_test * 100:.2f}%\")\n",
    "print(f\"Recall on Test Data: {recall_test * 100:.2f}%\")\n",
    "print(f\"F1-Score on Test Data: {f1_test * 100:.2f}%\")\n",
    "\n",
    "# Generate a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_binary, y_pred_test))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_binary, y_pred_test))\n",
    "\n",
    "# Save the trained XGBoost model\n",
    "joblib.dump(xgb_model, 'models_try/xgb_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class  weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using inverse frequency method\n",
    "\n",
    "class_weights = {\n",
    "    1: 1,     # Majority class 1\n",
    "    4: 1,     # Majority class 4\n",
    "    0: 3,     # Minority class 0\n",
    "    2: 2.5,   # Minority class 2\n",
    "    3: 2      # Minority class 3\n",
    "}\n",
    "\n",
    "\n",
    "class_weights_binary = {\n",
    "    0: 17,     # Minority class \n",
    "    1: 1,     # Majority class \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary target variable\n",
    "collision_free_data_upsampled['binary_target'] = collision_free_data_upsampled['action'].apply(lambda x: 1 if x in [1,3, 4] else 0)\n",
    "\n",
    "# Split the data into features and target\n",
    "X_binary = collision_free_data_upsampled.drop(columns=['action', 'binary_target'])\n",
    "y_binary = collision_free_data_upsampled['binary_target']\n",
    "\n",
    "# Split the dataset into train, test, and eval sets\n",
    "X_train_binary, X_temp_binary, y_train_binary, y_temp_binary = train_test_split(X_binary, y_binary, test_size=0.2, random_state=42)\n",
    "X_test_binary, X_eval_binary, y_test_binary, y_eval_binary = train_test_split(X_temp_binary, y_temp_binary, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train the binary classifier with class weights\n",
    "binary_rf_model = RandomForestClassifier(class_weight=class_weights_binary, random_state=42)\n",
    "binary_rf_model.fit(X_train_binary, y_train_binary)\n",
    "\n",
    "# Evaluate the binary classifier\n",
    "y_pred_binary_test = binary_rf_model.predict(X_test_binary)\n",
    "accuracy_binary_test = accuracy_score(y_test_binary, y_pred_binary_test)\n",
    "print(f\"Binary Classifier Accuracy on Test Data: {accuracy_binary_test * 100:.2f}%\")\n",
    "print(f\"Precision on Test Data: {precision_test * 100:.2f}%\")\n",
    "print(f\"Recall on Test Data: {recall_test * 100:.2f}%\")\n",
    "print(f\"F1-Score on Test Data: {f1_test * 100:.2f}%\")\n",
    "\n",
    "# Generate a detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_binary, y_pred_binary_test))\n",
    "\n",
    "# Generate a confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_binary, y_pred_binary_test))\n",
    "\n",
    "# Save the binary classifier\n",
    "joblib.dump(binary_rf_model, 'models_try/binary_rf_model_weights.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minor action classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for classes 0, 2,\n",
    "minor_data = collision_free_data_upsampled[collision_free_data_upsampled['binary_target'] == 0]\n",
    "#print(filtered_data[0:5])\n",
    "\n",
    "# Split the dataset into features and target\n",
    "X = minor_data.drop(columns=['action', 'binary_target'])\n",
    "y = minor_data['action']  # Multi-class target\n",
    "\n",
    "# Split into train, test, and eval sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_eval, y_test, y_eval = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a single Random Forest classifier\n",
    "rf_model_minor = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model_minor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_test = rf_model_minor.predict(X_test)\n",
    "y_pred_eval = rf_model_minor.predict(X_eval)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Evaluation Accuracy:\", accuracy_score(y_eval, y_pred_eval))\n",
    "print(\"\\nClassification Report on Test Data:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "# Save the binary classifier\n",
    "joblib.dump(rf_model_minor, 'models_try/minor_rf_model_upsampled.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "major action classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using all data for 1-3-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset saved as 'filtered_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def filter_data(file_path, labels_to_extract):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if the 'action' column exists\n",
    "    if 'action' not in df.columns:\n",
    "        raise ValueError(f\"The 'action' column is missing in the file: {file_path}\")\n",
    "    \n",
    "    # Filter the dataframe based on the action label\n",
    "    filtered_df = df[df['action'].isin(labels_to_extract)]\n",
    "    return filtered_df\n",
    "\n",
    "# Function to process multiple CSV files and merge the data\n",
    "def process_datasets(file_paths, labels_to_extract):\n",
    "    all_filtered_data = []\n",
    "    \n",
    "    # Process each dataset\n",
    "    for file_path in file_paths:\n",
    "        filtered_data = filter_data(file_path, labels_to_extract)\n",
    "        all_filtered_data.append(filtered_data)\n",
    "    \n",
    "    # Combine all the filtered data into a single dataframe\n",
    "    combined_data = pd.concat(all_filtered_data, ignore_index=True)\n",
    "    return combined_data\n",
    "\n",
    "# List of file paths (you can add/remove paths as needed)\n",
    "file_paths = [\n",
    "      # Add paths to your CSV files\n",
    "    'datasets_try/processed_features_collision_free.csv',\n",
    "    #'datasets_try/processed_features_all.csv',\n",
    "    # 'datasets_try/processed_features_el_cl.csv'\n",
    "    # 'dataset4.csv',\n",
    "    # 'dataset5.csv'\n",
    "]\n",
    "\n",
    "# Define labels to extract\n",
    "labels_to_extract = [1, 3,0, 4]\n",
    "\n",
    "# Process the datasets and create the new dataset\n",
    "new_dataset = process_datasets(file_paths, labels_to_extract)\n",
    "\n",
    "# Save the new dataset to a CSV file\n",
    "new_dataset.to_csv('datasets_try/processed_major.csv', index=False)\n",
    "\n",
    "print(\"Filtered dataset saved as 'filtered_dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    7872\n",
      "4    5842\n",
      "0    4294\n",
      "1    3461\n",
      "Name: action, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "major_data = pd.read_csv('datasets_try/major.csv')\n",
    "\n",
    "# Check the distribution of the 'action' column\n",
    "action_distribution =major_data['action'].value_counts()\n",
    "print(action_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    3461\n",
      "4    3461\n",
      "3    3461\n",
      "0    3461\n",
      "Name: action, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "slow_class = major_data[major_data['action'] == 4]\n",
    "idle_class = major_data[major_data['action'] == 1]\n",
    "fast_class =major_data[major_data['action'] == 3]\n",
    "left_class =major_data[major_data['action'] == 0]\n",
    "\n",
    "# Downsample majority class\n",
    "idle_downsampled = resample(idle_class, \n",
    "                                replace=True,    # Sample without replacement\n",
    "                                n_samples=3461,  # Match minority class size\n",
    "                                random_state=42)\n",
    "slow_downsampled = resample(slow_class, \n",
    "                                replace=True,    # Sample without replacement\n",
    "                                n_samples=3461,  # Match minority class size\n",
    "                                random_state=42)\n",
    "\n",
    "fast_downsampled = resample(fast_class, \n",
    "                                replace=True,    # Sample without replacement\n",
    "                                n_samples=3461,  # Match minority class size\n",
    "                                random_state=42)\n",
    "                        \n",
    "left_downsampled = resample(left_class, \n",
    "                                replace=True,    # Sample without replacement\n",
    "                                n_samples=3461,  # Match minority class size\n",
    "                                random_state=42)\n",
    "major_downsampled = pd.concat([idle_downsampled, slow_downsampled, fast_downsampled,left_downsampled])\n",
    "\n",
    "print(major_downsampled['action'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.763728323699422\n",
      "Evaluation Accuracy: 0.7595667870036101\n",
      "\n",
      "Classification Report on Test Data:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76       346\n",
      "           1       0.78      0.79      0.79       341\n",
      "           3       0.75      0.66      0.70       345\n",
      "           4       0.79      0.80      0.80       352\n",
      "\n",
      "    accuracy                           0.76      1384\n",
      "   macro avg       0.76      0.76      0.76      1384\n",
      "weighted avg       0.76      0.76      0.76      1384\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[278  14  25  29]\n",
      " [ 19 269  30  23]\n",
      " [ 55  40 228  22]\n",
      " [ 31  20  19 282]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['models_try/major_rf_model_down_upsampled.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Filter the data for classes 1 3and 4\n",
    "# major_data_down_upsampled = collision_free_data_up_downsampled[collision_free_data_up_downsampled['binary_target'] == 1]\n",
    "# #print(filtered_data[0:5])\n",
    "\n",
    "# Split the dataset into features and target\n",
    "# X = new_dataset.drop(columns=['action', 'binary_target'])\n",
    "X = major_downsampled.drop(columns=['action'])\n",
    "y = major_downsampled['action']  # Multi-class target\n",
    "\n",
    "# Split into train, test, and eval sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_test, X_eval, y_test, y_eval = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a single Random Forest classifier\n",
    "rf_model_major = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model_major.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_test = rf_model_major.predict(X_test)\n",
    "y_pred_eval = rf_model_major.predict(X_eval)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Evaluation Accuracy:\", accuracy_score(y_eval, y_pred_eval))\n",
    "print(\"\\nClassification Report on Test Data:\\n\", classification_report(y_test, y_pred_test))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "# Save the binary classifier\n",
    "joblib.dump(rf_model_major, 'models_try/major_rf_model_down_upsampled.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
